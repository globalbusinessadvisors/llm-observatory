# Application Configuration
APP_NAME="LLM Observatory Chat API"
ENVIRONMENT=development
DEBUG=true
HOST=0.0.0.0
PORT=8000

# CORS
CORS_ORIGINS=http://localhost:3000,http://localhost:8080

# Database
DATABASE_URL=postgresql://llm_observatory_app:change_me_in_production@localhost:5432/llm_observatory
DB_POOL_SIZE=5
DB_MAX_OVERFLOW=10
DB_ECHO=false

# Redis
REDIS_URL=redis://:redis_password@localhost:6379/0
REDIS_CACHE_TTL=3600

# OpenAI
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_ORG_ID=
OPENAI_DEFAULT_MODEL=gpt-4

# Anthropic
ANTHROPIC_API_KEY=your_anthropic_api_key_here
ANTHROPIC_DEFAULT_MODEL=claude-3-sonnet-20240229

# Azure OpenAI
AZURE_OPENAI_API_KEY=
AZURE_OPENAI_ENDPOINT=
AZURE_OPENAI_API_VERSION=2024-02-01

# LLM Configuration
DEFAULT_PROVIDER=openai
MAX_TOKENS=4096
TEMPERATURE=0.7
ENABLE_STREAMING=true

# Observability
OTLP_COLLECTOR_URL=http://localhost:4318
ENABLE_TRACING=true
ENABLE_METRICS=true

# Logging
LOG_LEVEL=INFO
LOG_FORMAT=json

# Security
SECRET_KEY=change_me_to_a_random_secret_key_in_production
JWT_SECRET=change_me_to_a_random_jwt_secret

# PII Protection
ENABLE_PII_DETECTION=true
REDACT_PII=true
