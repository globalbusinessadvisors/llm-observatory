version: '3.8'

services:
  # Chat API Service
  chat-api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: llm-observatory-chat-api
    restart: unless-stopped
    ports:
      - "${CHAT_API_PORT:-8000}:8000"
    environment:
      # Application
      APP_NAME: "LLM Observatory Chat API"
      ENVIRONMENT: ${ENVIRONMENT:-development}
      DEBUG: ${DEBUG:-false}
      HOST: 0.0.0.0
      PORT: 8000

      # CORS
      CORS_ORIGINS: ${CORS_ORIGINS:-http://localhost:3000,http://localhost:8080}

      # Database
      DATABASE_URL: postgresql://${DB_APP_USER:-llm_observatory_app}:${DB_APP_PASSWORD:-change_me_in_production}@timescaledb:5432/${DB_NAME:-llm_observatory}
      DB_POOL_SIZE: ${DB_POOL_SIZE:-5}
      DB_MAX_OVERFLOW: ${DB_MAX_OVERFLOW:-10}
      DB_ECHO: ${DB_QUERY_LOGGING:-false}

      # Redis
      REDIS_URL: redis://:${REDIS_PASSWORD:-redis_password}@redis:6379/0
      REDIS_CACHE_TTL: ${CACHE_DEFAULT_TTL:-3600}

      # OpenAI
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      OPENAI_ORG_ID: ${OPENAI_ORG_ID:-}
      OPENAI_DEFAULT_MODEL: ${OPENAI_DEFAULT_MODEL:-gpt-4}
      OPENAI_MAX_RETRIES: 3
      OPENAI_TIMEOUT: 60

      # Anthropic
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY:-}
      ANTHROPIC_DEFAULT_MODEL: claude-3-sonnet-20240229

      # Azure OpenAI
      AZURE_OPENAI_API_KEY: ${AZURE_OPENAI_API_KEY:-}
      AZURE_OPENAI_ENDPOINT: ${AZURE_OPENAI_ENDPOINT:-}
      AZURE_OPENAI_API_VERSION: ${AZURE_OPENAI_API_VERSION:-2024-02-01}

      # LLM Configuration
      DEFAULT_PROVIDER: ${DEFAULT_PROVIDER:-openai}
      MAX_TOKENS: 4096
      TEMPERATURE: 0.7
      ENABLE_STREAMING: true

      # Rate Limiting
      RATE_LIMIT_ENABLED: ${RATE_LIMIT_ENABLED:-true}
      RATE_LIMIT_REQUESTS: ${RATE_LIMIT_REQUESTS:-100}
      RATE_LIMIT_WINDOW: ${RATE_LIMIT_WINDOW:-60}

      # Observability
      OTLP_COLLECTOR_URL: ${OTLP_COLLECTOR_URL:-http://collector:4318}
      ENABLE_TRACING: ${ENABLE_TRACING:-true}
      ENABLE_METRICS: ${ENABLE_METRICS:-true}

      # Logging
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      LOG_FORMAT: ${LOG_FORMAT:-json}

      # Security
      SECRET_KEY: ${SECRET_KEY:-change_me_to_a_random_secret_key_in_production}
      JWT_SECRET: ${JWT_SECRET:-change_me_to_a_random_jwt_secret}

      # PII Protection
      ENABLE_PII_DETECTION: ${ENABLE_PII_DETECTION:-true}
      REDACT_PII: ${REDACT_PII:-true}

    depends_on:
      - timescaledb
      - redis
    networks:
      - llm-observatory-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s

  # PostgreSQL (TimescaleDB) - Required by chat-api
  timescaledb:
    image: timescale/timescaledb:2.14.2-pg16
    container_name: llm-observatory-db
    restart: unless-stopped
    ports:
      - "${DB_PORT:-5432}:5432"
    environment:
      POSTGRES_USER: ${DB_USER:-postgres}
      POSTGRES_PASSWORD: ${DB_PASSWORD:-postgres}
      POSTGRES_DB: ${DB_NAME:-llm_observatory}
      TIMESCALEDB_TELEMETRY: "off"
    volumes:
      - timescaledb_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER:-postgres} -d ${DB_NAME:-llm_observatory}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - llm-observatory-network
    command: >
      postgres
      -c shared_preload_libraries=timescaledb
      -c max_connections=200

  # Redis - Required by chat-api
  redis:
    image: redis:7.2-alpine
    container_name: llm-observatory-redis
    restart: unless-stopped
    ports:
      - "${REDIS_PORT:-6379}:6379"
    environment:
      REDIS_PASSWORD: ${REDIS_PASSWORD:-redis_password}
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
      start_period: 10s
    networks:
      - llm-observatory-network
    command: >
      redis-server
      --requirepass ${REDIS_PASSWORD:-redis_password}
      --maxmemory 256mb
      --maxmemory-policy allkeys-lru
      --appendonly yes
      --appendfsync everysec

networks:
  llm-observatory-network:
    driver: bridge
    name: llm-observatory-network

volumes:
  timescaledb_data:
    name: llm-observatory-db-data
  redis_data:
    name: llm-observatory-redis-data
