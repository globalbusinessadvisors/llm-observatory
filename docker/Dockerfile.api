# ============================================================================
# LLM Observatory - API Service Production Dockerfile
# ============================================================================
#
# This multi-stage Dockerfile builds the API service with:
# - REST and GraphQL endpoints
# - JWT authentication
# - CORS configuration
# - Security hardening (non-root user, minimal attack surface)
# - Optimized binary size with static linking
#
# Build:  docker build -f docker/Dockerfile.api -t llm-observatory-api:latest .
# Run:    docker run -p 8080:8080 -p 9090:9090 llm-observatory-api:latest
#
# ============================================================================

# ----------------------------------------------------------------------------
# Stage 1: Chef - Cache dependencies for faster rebuilds
# ----------------------------------------------------------------------------
FROM rust:1.83-bookworm AS chef

# Install cargo-chef for dependency caching
RUN cargo install cargo-chef --version 0.1.67

WORKDIR /app

# ----------------------------------------------------------------------------
# Stage 2: Planner - Generate recipe file for dependencies
# ----------------------------------------------------------------------------
FROM chef AS planner

# Copy entire workspace to analyze dependencies
COPY . .

# Generate recipe.json with all workspace dependencies
RUN cargo chef prepare --recipe-path recipe.json

# ----------------------------------------------------------------------------
# Stage 3: Builder - Build dependencies (cached layer)
# ----------------------------------------------------------------------------
FROM chef AS builder

# Install required system dependencies for building
RUN apt-get update && apt-get install -y \
    pkg-config \
    libssl-dev \
    libpq-dev \
    protobuf-compiler \
    && rm -rf /var/lib/apt/lists/*

# Copy dependency recipe
COPY --from=planner /app/recipe.json recipe.json

# Build dependencies (this layer is cached unless dependencies change)
RUN cargo chef cook --release --recipe-path recipe.json

# ----------------------------------------------------------------------------
# Stage 4: Application Builder - Build the API service
# ----------------------------------------------------------------------------
FROM builder AS app-builder

# Copy the entire workspace
COPY . .

# Build only the API service binary with optimizations
# - Release mode with LTO and codegen-units=1 for smaller binary
# - Strip symbols for additional size reduction
RUN cargo build --release --bin llm-observatory-api && \
    strip /app/target/release/llm-observatory-api

# Verify the binary was built successfully
RUN test -f /app/target/release/llm-observatory-api || \
    (echo "ERROR: Binary not found!" && exit 1)

# ----------------------------------------------------------------------------
# Stage 5: Runtime - Minimal production image
# ----------------------------------------------------------------------------
FROM debian:bookworm-slim AS runtime

# Install only runtime dependencies
RUN apt-get update && apt-get install -y \
    ca-certificates \
    libpq5 \
    libssl3 \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# Create non-root user for security
RUN groupadd -r -g 1000 apiuser && \
    useradd -r -u 1000 -g apiuser -s /sbin/nologin -c "API User" apiuser

# Create necessary directories with proper permissions
RUN mkdir -p /app /app/config /var/log/api && \
    chown -R apiuser:apiuser /app /var/log/api

WORKDIR /app

# Copy the compiled binary from builder
COPY --from=app-builder --chown=apiuser:apiuser \
    /app/target/release/llm-observatory-api \
    /app/llm-observatory-api

# Copy configuration files (if they exist)
COPY --chown=apiuser:apiuser docker/api-config*.yml /app/config/ 2>/dev/null || true

# Security: Switch to non-root user
USER apiuser

# Expose ports
# 8080 - API service (REST/GraphQL)
# 9090 - Metrics/Prometheus endpoint
EXPOSE 8080 9090

# Environment variables (can be overridden)
ENV RUST_LOG=info \
    RUST_BACKTRACE=1 \
    APP_HOST=0.0.0.0 \
    APP_PORT=8080 \
    METRICS_PORT=9090 \
    ENVIRONMENT=production

# Health check configuration
# Checks the /health endpoint every 30 seconds
HEALTHCHECK --interval=30s --timeout=5s --start-period=10s --retries=3 \
    CMD ["/app/llm-observatory-api", "health-check"] || exit 1

# Set the entrypoint to the API binary
ENTRYPOINT ["/app/llm-observatory-api"]

# Default command (can be overridden)
CMD ["serve"]

# ============================================================================
# Security Labels & Metadata
# ============================================================================
LABEL maintainer="LLM Observatory Contributors" \
      org.opencontainers.image.title="LLM Observatory API" \
      org.opencontainers.image.description="REST and GraphQL API for LLM Observatory" \
      org.opencontainers.image.vendor="LLM Observatory" \
      org.opencontainers.image.licenses="Apache-2.0" \
      org.opencontainers.image.source="https://github.com/llm-observatory/llm-observatory" \
      security.non-root="true" \
      security.readonly-rootfs="false"
